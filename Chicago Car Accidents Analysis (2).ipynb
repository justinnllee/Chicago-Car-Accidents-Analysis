{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chicago Car Accidents Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Justin Lee\n",
    "\n",
    "This notebook is prepared for the Vehicle Safety Board of Chicago. This board aims to understand how often driver accidents are caused by a failure to yield. The purpose of this analysis is to build a model that accurately predicts how often failure to yield accidents are actually due to a failure to yield."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is from the Chicago Data Portal. This data contains information about people involved in a crash and if any injuries were sustained. Each record corresponds to an occupant in a vehicle listed in the Crash dataset. Some people involved in a crash may not have been an occupant in a motor vehicle, but may have been a pedestrian, bicyclist, or using another non-motor vehicle mode of transportation. Person data can be linked with the Crash and Vehicle dataset using the “CRASH_RECORD_ID” field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import any relevant library\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinlee/anaconda3/envs/learn-env/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3145: DtypeWarning: Columns (19,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERSON_ID</th>\n",
       "      <th>PERSON_TYPE</th>\n",
       "      <th>CRASH_RECORD_ID</th>\n",
       "      <th>VEHICLE_ID</th>\n",
       "      <th>CRASH_DATE</th>\n",
       "      <th>SEAT_NO</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>ZIPCODE</th>\n",
       "      <th>SEX</th>\n",
       "      <th>...</th>\n",
       "      <th>EMS_RUN_NO</th>\n",
       "      <th>DRIVER_ACTION</th>\n",
       "      <th>DRIVER_VISION</th>\n",
       "      <th>PHYSICAL_CONDITION</th>\n",
       "      <th>PEDPEDAL_ACTION</th>\n",
       "      <th>PEDPEDAL_VISIBILITY</th>\n",
       "      <th>PEDPEDAL_LOCATION</th>\n",
       "      <th>BAC_RESULT</th>\n",
       "      <th>BAC_RESULT VALUE</th>\n",
       "      <th>CELL_PHONE_USE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O749947</td>\n",
       "      <td>DRIVER</td>\n",
       "      <td>81dc0de2ed92aa62baccab641fa377be7feb1cc47e6554...</td>\n",
       "      <td>834816.0</td>\n",
       "      <td>09/28/2019 03:30:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>IL</td>\n",
       "      <td>60651</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TEST NOT OFFERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>O871921</td>\n",
       "      <td>DRIVER</td>\n",
       "      <td>af84fb5c8d996fcd3aefd36593c3a02e6e7509eeb27568...</td>\n",
       "      <td>827212.0</td>\n",
       "      <td>04/13/2020 10:50:00 PM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHICAGO</td>\n",
       "      <td>IL</td>\n",
       "      <td>60620</td>\n",
       "      <td>M</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NONE</td>\n",
       "      <td>NOT OBSCURED</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TEST NOT OFFERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O10018</td>\n",
       "      <td>DRIVER</td>\n",
       "      <td>71162af7bf22799b776547132ebf134b5b438dcf3dac6b...</td>\n",
       "      <td>9579.0</td>\n",
       "      <td>11/01/2015 05:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IMPROPER BACKING</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TEST NOT OFFERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>O10038</td>\n",
       "      <td>DRIVER</td>\n",
       "      <td>c21c476e2ccc41af550b5d858d22aaac4ffc88745a1700...</td>\n",
       "      <td>9598.0</td>\n",
       "      <td>11/01/2015 08:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TEST NOT OFFERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>O10039</td>\n",
       "      <td>DRIVER</td>\n",
       "      <td>eb390a4c8e114c69488f5fb8a097fe629f5a92fd528cf4...</td>\n",
       "      <td>9600.0</td>\n",
       "      <td>11/01/2015 10:15:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>UNKNOWN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TEST NOT OFFERED</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PERSON_ID PERSON_TYPE                                    CRASH_RECORD_ID  \\\n",
       "0   O749947      DRIVER  81dc0de2ed92aa62baccab641fa377be7feb1cc47e6554...   \n",
       "1   O871921      DRIVER  af84fb5c8d996fcd3aefd36593c3a02e6e7509eeb27568...   \n",
       "2    O10018      DRIVER  71162af7bf22799b776547132ebf134b5b438dcf3dac6b...   \n",
       "3    O10038      DRIVER  c21c476e2ccc41af550b5d858d22aaac4ffc88745a1700...   \n",
       "4    O10039      DRIVER  eb390a4c8e114c69488f5fb8a097fe629f5a92fd528cf4...   \n",
       "\n",
       "   VEHICLE_ID              CRASH_DATE  SEAT_NO     CITY STATE ZIPCODE SEX  \\\n",
       "0    834816.0  09/28/2019 03:30:00 AM      NaN  CHICAGO    IL   60651   M   \n",
       "1    827212.0  04/13/2020 10:50:00 PM      NaN  CHICAGO    IL   60620   M   \n",
       "2      9579.0  11/01/2015 05:00:00 AM      NaN      NaN   NaN     NaN   X   \n",
       "3      9598.0  11/01/2015 08:00:00 AM      NaN      NaN   NaN     NaN   X   \n",
       "4      9600.0  11/01/2015 10:15:00 AM      NaN      NaN   NaN     NaN   X   \n",
       "\n",
       "   ...  EMS_RUN_NO     DRIVER_ACTION DRIVER_VISION PHYSICAL_CONDITION  \\\n",
       "0  ...         NaN           UNKNOWN       UNKNOWN            UNKNOWN   \n",
       "1  ...         NaN              NONE  NOT OBSCURED             NORMAL   \n",
       "2  ...         NaN  IMPROPER BACKING       UNKNOWN            UNKNOWN   \n",
       "3  ...         NaN           UNKNOWN       UNKNOWN            UNKNOWN   \n",
       "4  ...         NaN           UNKNOWN       UNKNOWN            UNKNOWN   \n",
       "\n",
       "  PEDPEDAL_ACTION PEDPEDAL_VISIBILITY PEDPEDAL_LOCATION        BAC_RESULT  \\\n",
       "0             NaN                 NaN               NaN  TEST NOT OFFERED   \n",
       "1             NaN                 NaN               NaN  TEST NOT OFFERED   \n",
       "2             NaN                 NaN               NaN  TEST NOT OFFERED   \n",
       "3             NaN                 NaN               NaN  TEST NOT OFFERED   \n",
       "4             NaN                 NaN               NaN  TEST NOT OFFERED   \n",
       "\n",
       "  BAC_RESULT VALUE CELL_PHONE_USE  \n",
       "0              NaN            NaN  \n",
       "1              NaN            NaN  \n",
       "2              NaN            NaN  \n",
       "3              NaN            NaN  \n",
       "4              NaN            NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in our dataframe\n",
    "df = pd.read_csv('traffic_crashes.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VEHICLE_ID</th>\n",
       "      <th>SEAT_NO</th>\n",
       "      <th>AGE</th>\n",
       "      <th>BAC_RESULT VALUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.964730e+06</td>\n",
       "      <td>405468.00000</td>\n",
       "      <td>1.422362e+06</td>\n",
       "      <td>2216.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.441771e+05</td>\n",
       "      <td>4.16478</td>\n",
       "      <td>3.792867e+01</td>\n",
       "      <td>0.171340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.491011e+05</td>\n",
       "      <td>2.21842</td>\n",
       "      <td>1.708682e+01</td>\n",
       "      <td>0.103318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-1.770000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.680342e+05</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>2.500000e+01</td>\n",
       "      <td>0.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.363780e+05</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>3.500000e+01</td>\n",
       "      <td>0.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.422413e+06</td>\n",
       "      <td>6.00000</td>\n",
       "      <td>5.000000e+01</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.900249e+06</td>\n",
       "      <td>12.00000</td>\n",
       "      <td>1.100000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         VEHICLE_ID       SEAT_NO           AGE  BAC_RESULT VALUE\n",
       "count  1.964730e+06  405468.00000  1.422362e+06       2216.000000\n",
       "mean   9.441771e+05       4.16478  3.792867e+01          0.171340\n",
       "std    5.491011e+05       2.21842  1.708682e+01          0.103318\n",
       "min    2.000000e+00       1.00000 -1.770000e+02          0.000000\n",
       "25%    4.680342e+05       3.00000  2.500000e+01          0.127500\n",
       "50%    9.363780e+05       3.00000  3.500000e+01          0.170000\n",
       "75%    1.422413e+06       6.00000  5.000000e+01          0.220000\n",
       "max    1.900249e+06      12.00000  1.100000e+02          1.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2005877 entries, 0 to 2005876\n",
      "Data columns (total 29 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   PERSON_ID              object \n",
      " 1   PERSON_TYPE            object \n",
      " 2   CRASH_RECORD_ID        object \n",
      " 3   VEHICLE_ID             float64\n",
      " 4   CRASH_DATE             object \n",
      " 5   SEAT_NO                float64\n",
      " 6   CITY                   object \n",
      " 7   STATE                  object \n",
      " 8   ZIPCODE                object \n",
      " 9   SEX                    object \n",
      " 10  AGE                    float64\n",
      " 11  DRIVERS_LICENSE_STATE  object \n",
      " 12  DRIVERS_LICENSE_CLASS  object \n",
      " 13  SAFETY_EQUIPMENT       object \n",
      " 14  AIRBAG_DEPLOYED        object \n",
      " 15  EJECTION               object \n",
      " 16  INJURY_CLASSIFICATION  object \n",
      " 17  HOSPITAL               object \n",
      " 18  EMS_AGENCY             object \n",
      " 19  EMS_RUN_NO             object \n",
      " 20  DRIVER_ACTION          object \n",
      " 21  DRIVER_VISION          object \n",
      " 22  PHYSICAL_CONDITION     object \n",
      " 23  PEDPEDAL_ACTION        object \n",
      " 24  PEDPEDAL_VISIBILITY    object \n",
      " 25  PEDPEDAL_LOCATION      object \n",
      " 26  BAC_RESULT             object \n",
      " 27  BAC_RESULT VALUE       float64\n",
      " 28  CELL_PHONE_USE         object \n",
      "dtypes: float64(4), object(25)\n",
      "memory usage: 443.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PERSON_ID                      0\n",
       "PERSON_TYPE                    0\n",
       "CRASH_RECORD_ID                0\n",
       "VEHICLE_ID                 41147\n",
       "CRASH_DATE                     0\n",
       "SEAT_NO                  1600409\n",
       "CITY                      545926\n",
       "STATE                     523661\n",
       "ZIPCODE                   662176\n",
       "SEX                        33887\n",
       "AGE                       583515\n",
       "DRIVERS_LICENSE_STATE     831360\n",
       "DRIVERS_LICENSE_CLASS    1030131\n",
       "SAFETY_EQUIPMENT            5603\n",
       "AIRBAG_DEPLOYED            39600\n",
       "EJECTION                   25264\n",
       "INJURY_CLASSIFICATION        757\n",
       "HOSPITAL                 1682144\n",
       "EMS_AGENCY               1806123\n",
       "EMS_RUN_NO               1972477\n",
       "DRIVER_ACTION             409055\n",
       "DRIVER_VISION             409691\n",
       "PHYSICAL_CONDITION        407959\n",
       "PEDPEDAL_ACTION          1966543\n",
       "PEDPEDAL_VISIBILITY      1966613\n",
       "PEDPEDAL_LOCATION        1966542\n",
       "BAC_RESULT                408136\n",
       "BAC_RESULT VALUE         2003661\n",
       "CELL_PHONE_USE           2004717\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore number of null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NONE                                 568041\n",
       "UNKNOWN                              406729\n",
       "FAILED TO YIELD                      145118\n",
       "OTHER                                143764\n",
       "FOLLOWED TOO CLOSELY                  93147\n",
       "IMPROPER BACKING                      46726\n",
       "IMPROPER TURN                         42036\n",
       "IMPROPER LANE CHANGE                  41055\n",
       "IMPROPER PASSING                      35904\n",
       "DISREGARDED CONTROL DEVICES           28288\n",
       "TOO FAST FOR CONDITIONS               23312\n",
       "WRONG WAY/SIDE                         6449\n",
       "IMPROPER PARKING                       5856\n",
       "OVERCORRECTED                          3230\n",
       "EVADING POLICE VEHICLE                 2473\n",
       "CELL PHONE USE OTHER THAN TEXTING      2313\n",
       "EMERGENCY VEHICLE ON CALL              1493\n",
       "TEXTING                                 626\n",
       "STOPPED SCHOOL BUS                      193\n",
       "LICENSE RESTRICTIONS                     69\n",
       "Name: DRIVER_ACTION, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore our target variable counts\n",
    "df['DRIVER_ACTION'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prepare our analysis, we must prepare our data into binary classification. First we'll drop any unnecessary columns for our analylsis. These below columns will get dropped because they will not actually help us with our analysis. We'll handle the NONE, OTHER and UNKNOWN values to be set to null. Then we will create a new binary column where 1 represents FAILED TO YIELD and 0 represents all other driver actions. Finally, we will one-hot encode our categorical columns into numerical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to drop\n",
    "columns_to_drop = ['HOSPITAL', 'EMS_AGENCY', 'EMS_RUN_NO', 'PERSON_ID', 'CRASH_RECORD_ID', 'VEHICLE_ID',\n",
    "                  'PERSON_TYPE', 'CRASH_DATE', 'CITY', 'STATE', 'ZIPCODE','SEAT_NO', 'SEX', 'AGE', 'DRIVERS_LICENSE_STATE', \n",
    "                   'DRIVERS_LICENSE_CLASS', 'SAFETY_EQUIPMENT', 'AIRBAG_DEPLOYED', 'EJECTION', 'INJURY_CLASSIFICATION']\n",
    "\n",
    "# Drop each column individually using a loop\n",
    "for col in columns_to_drop:\n",
    "    if col in df.columns:\n",
    "        df.drop(columns=[col], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NONE, OTHER and UNKOWN values to null\n",
    "df['DRIVER_ACTION'] = df['DRIVER_ACTION'].replace(['UNKNOWN', 'NONE', 'OTHER'], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1527589"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify how many values are now missing\n",
    "df['DRIVER_ACTION'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a binary classification target, 1 = FAILED TO YIELD and 0 = all other values\n",
    "df['target'] = (df['DRIVER_ACTION'] == 'FAILED TO YIELD').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because there are lot of missing values, we will group these values into not failed to yield\n",
    "# This will help us not lose any data\n",
    "df['DRIVER_ACTION'] = df['DRIVER_ACTION'].fillna('NOT_FAILED_TO_YIELD')\n",
    "df['target'] = df['target'].fillna(0)  # Ensure all missing values are assigned to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because our target variable is cleaned, we can drop the DRIVER_ACTION column and select relevant features\n",
    "X = df.drop(columns=['DRIVER_ACTION', 'target'])\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1860759\n",
       "1     145118\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This shows us the class imbalance of our inital dataset\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['DRIVER_VISION', 'PHYSICAL_CONDITION', 'PEDPEDAL_ACTION',\n",
      "       'PEDPEDAL_VISIBILITY', 'PEDPEDAL_LOCATION', 'BAC_RESULT',\n",
      "       'CELL_PHONE_USE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode our categorcial before our modeling\n",
    "# Identify categorical columns\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding, get_dummies to convert categorical columns into numerical ones\n",
    "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logisic regression is a good baseline model because it is simple, interpretable and efficient to provide a strong foundation for comparison. This will help us determine what improvements will be needed when iterating on our model. We'll first impute our null values to fill with mode values so that we can maintain the integrity/size of our data. Then we will scale our features because logistic regression is sensitive to feature magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset to evaluate the model's generalization ability\n",
    "# stratify = y helps to ensure the \"FAILED TO YIELD\" accidents is maintained in both train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling any missing NaN values in X_test or X_train or else this will roadblock us in modeling\n",
    "# This imputer fills NaNs with the most frequent value (mode) for categorical and median for numerical\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "# Apply imputation to both X_train and X_test\n",
    "X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=42)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a baseline logistic regression model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model - Logistic Regression Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9276601790735238\n",
      "Confusion Matrix:\n",
      " [[372149      3]\n",
      " [ 29018      6]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96    372152\n",
      "           1       0.67      0.00      0.00     29024\n",
      "\n",
      "    accuracy                           0.93    401176\n",
      "   macro avg       0.80      0.50      0.48    401176\n",
      "weighted avg       0.91      0.93      0.89    401176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the baseline model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model achieved 93% accuracy which looks great but this class is also very imbalanced.\n",
    "\n",
    "Our model had 372,149 true negatives which is the total correctly predicated \"NOT FAILED TO YIELD\". We had 3 false positives which are the incorrectly predicted \"FAILED TO YIELD\" cases but were actually \"NOT FAILED TO YIELD\". Our model missed 29,018 actual \"FAILED TO YIELD\" cases. Our model only correctly predicted 6 \"FAILED TO YIELD\" cases.\n",
    "\n",
    "Precision for Class 1 (\"FAILED TO YIELD\") was 0.67 but this is misleading due to only having 6 true positives. We also had a recall score of 0 and f1-score of 0. A recall of 0 means that our model is not detecting actual \"FAILED TO YIELD\" cases at all. Out of 29,024 actual \"FAILED TO YIELD\" cases it only found 6. This means it failes to identify accidents caused by \"FAILED TO YIELD\". Since our recall is 0, it makes sense that our f1-score is also 0. Our baseline model is completely missing the \"FAILED TO YIELD\" category.\n",
    "\n",
    "Our baseline model is heavily influenced by class imbalance. We are seeing 372,152 \"NOT FAILED TO YIELD\" cases versus only 29,024 \"FAILED TO YIELD\" cases. Since \"NOT FAILED TO YIELD\" dominates the data, the model learns to always predict the majority class (0) because it minimizes overall errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use statsmodels to understand the statistical significance and interpretability of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.239461\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justinlee/anaconda3/envs/learn-env/lib/python3.8/site-packages/statsmodels/base/model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 target   No. Observations:              1604701\n",
      "Model:                          Logit   Df Residuals:                  1604639\n",
      "Method:                           MLE   Df Model:                           61\n",
      "Date:                Sun, 02 Mar 2025   Pseudo R-squ.:                 0.07781\n",
      "Time:                        19:17:14   Log-Likelihood:            -3.8426e+05\n",
      "converged:                      False   LL-Null:                   -4.1669e+05\n",
      "Covariance Type:            nonrobust   LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.7097      2.164     -1.715      0.086      -7.950       0.531\n",
      "x1            -0.0039      0.004     -0.878      0.380      -0.013       0.005\n",
      "x2             0.2478      0.006     44.191      0.000       0.237       0.259\n",
      "x3             0.0496      0.003     17.132      0.000       0.044       0.055\n",
      "x4             0.1304      0.003     37.993      0.000       0.124       0.137\n",
      "x5             0.0483      0.002     20.074      0.000       0.044       0.053\n",
      "x6             0.0525      0.002     22.666      0.000       0.048       0.057\n",
      "x7             0.5488      0.012     46.839      0.000       0.526       0.572\n",
      "x8             3.1476      0.085     36.973      0.000       2.981       3.315\n",
      "x9             0.6252      0.015     40.525      0.000       0.595       0.655\n",
      "x10            0.4341      0.009     46.998      0.000       0.416       0.452\n",
      "x11            0.0329      0.002     15.190      0.000       0.029       0.037\n",
      "x12            0.1325      0.004     36.826      0.000       0.125       0.140\n",
      "x13            3.2067      0.085     37.909      0.000       3.041       3.373\n",
      "x14            0.3173      0.008     38.703      0.000       0.301       0.333\n",
      "x15           -0.0094      0.004     -2.151      0.032      -0.018      -0.001\n",
      "x16            0.0077      0.003      2.488      0.013       0.002       0.014\n",
      "x17           -0.0236      0.005     -4.988      0.000      -0.033      -0.014\n",
      "x18            0.0119      0.005      2.406      0.016       0.002       0.022\n",
      "x19            0.0036      0.003      1.242      0.214      -0.002       0.009\n",
      "x20            0.0086      0.003      3.019      0.003       0.003       0.014\n",
      "x21           -0.0050      0.004     -1.315      0.189      -0.013       0.002\n",
      "x22            0.1152      0.032      3.633      0.000       0.053       0.177\n",
      "x23           -0.0012      0.004     -0.273      0.785      -0.010       0.007\n",
      "x24           -0.0095      0.005     -1.970      0.049      -0.019   -4.87e-05\n",
      "x25            0.0726      0.028      2.581      0.010       0.017       0.128\n",
      "x26            0.0321      0.004      8.749      0.000       0.025       0.039\n",
      "x27           -0.0127      0.005     -2.647      0.008      -0.022      -0.003\n",
      "x28           -0.0107      0.005     -2.109      0.035      -0.021      -0.001\n",
      "x29           -0.0138      0.005     -2.797      0.005      -0.023      -0.004\n",
      "x30           -0.0791      0.009     -8.557      0.000      -0.097      -0.061\n",
      "x31            0.0211      0.002      9.379      0.000       0.017       0.025\n",
      "x32           -0.0036      0.004     -1.027      0.304      -0.010       0.003\n",
      "x33           -0.0497      0.007     -7.287      0.000      -0.063      -0.036\n",
      "x34           -0.0029      0.004     -0.760      0.447      -0.010       0.005\n",
      "x35           -0.0308      0.006     -5.095      0.000      -0.043      -0.019\n",
      "x36           -0.0216      0.006     -3.873      0.000      -0.033      -0.011\n",
      "x37           -0.0009      0.003     -0.323      0.747      -0.007       0.005\n",
      "x38           -0.0865    585.362     -0.000      1.000   -1147.375    1147.202\n",
      "x39           -0.0009      0.003     -0.261      0.794      -0.008       0.006\n",
      "x40           -0.0640      0.012     -5.206      0.000      -0.088      -0.040\n",
      "x41           -0.0015      0.004     -0.420      0.675      -0.009       0.006\n",
      "x42            0.0073      0.003      2.563      0.010       0.002       0.013\n",
      "x43            0.0049      0.003      1.897      0.058      -0.000       0.010\n",
      "x44           -0.0528      0.007     -7.318      0.000      -0.067      -0.039\n",
      "x45           -0.0482    306.409     -0.000      1.000    -600.599     600.502\n",
      "x46           -0.0451      0.006     -7.159      0.000      -0.057      -0.033\n",
      "x47           -0.0240      0.008     -2.847      0.004      -0.040      -0.007\n",
      "x48           -0.0119      0.010     -1.224      0.221      -0.031       0.007\n",
      "x49           -0.0098      0.005     -1.947      0.052      -0.020    6.61e-05\n",
      "x50           -0.0282      0.007     -3.857      0.000      -0.043      -0.014\n",
      "x51           -0.0107      0.006     -1.897      0.058      -0.022       0.000\n",
      "x52           -0.0011      0.004     -0.260      0.795      -0.009       0.007\n",
      "x53           -0.0456      0.010     -4.469      0.000      -0.066      -0.026\n",
      "x54            0.0212      0.010      2.182      0.029       0.002       0.040\n",
      "x55           -0.0055      0.005     -1.023      0.306      -0.016       0.005\n",
      "x56           -0.0099      0.006     -1.632      0.103      -0.022       0.002\n",
      "x57           -0.0262      0.007     -3.747      0.000      -0.040      -0.013\n",
      "x58            0.0051      0.003      1.874      0.061      -0.000       0.010\n",
      "x59           -0.0111      0.003     -3.697      0.000      -0.017      -0.005\n",
      "x60            0.0061      0.004      1.454      0.146      -0.002       0.014\n",
      "x61            0.0031      0.003      1.188      0.235      -0.002       0.008\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Add an intercept to X_train\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# Fit the logistic regression model\n",
    "model_sm = sm.Logit(y_train, X_train_sm)\n",
    "result = model_sm.fit()\n",
    "\n",
    "# Print the summary\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall our model is not detecting meaningful relationships between features and \"FAILED TO YIELD\". Pseudo R-squared compares the log-likelihood of our model versus a null model (a model with no predictors) so a lower score suggests a model doesn't explain much variation. Our pseudo R-squared value of 0.07781 means our model doesn't explain much variation in the outcome. Logistic regression maximizes the log-likelihood to find the best-fit model, so a higher value suggests a better fit. Our log-likelihood means of -3.8426e+05 suggests a week model, which could be due to a large class imbalance making our model predict mostly 0s - inflating accuracy but weakening predictive power. All p-values are high, meaning features may not be relevant predictors. This model also did not converge which is likely due to our high class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to balance the dataset. As we saw earlier our target column had one class's value almost be 10x the amount of values of the other class. For this reason we will balance the dataset using an under sampling technique. Under sampling removes excess majority class samples and ensures the model learns from real data only, it helps the model focus to learn from minority class cases instead of predicting \"NOT FAILED TO YIELD\" cases, and it would help us increase our F1-score as we strive for a healthy balance between recall and precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New class distribution after undersampling:\n",
      " 1    116094\n",
      "0    116094\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Apply random under sampling\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the new class distribution\n",
    "print(\"New class distribution after undersampling:\\n\", y_train_resampled.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=42)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train our new model on under sampled data\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the original dataset\n",
    "y_pred_undersampled = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Logistic Regression Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will test our model on the original (imbalanced) test set to see if recall, precision and f1-score have improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.29737322272518796\n",
      "Confusion Matrix:\n",
      " [[ 90673 281479]\n",
      " [   398  28626]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.24      0.39    372152\n",
      "           1       0.09      0.99      0.17     29024\n",
      "\n",
      "    accuracy                           0.30    401176\n",
      "   macro avg       0.54      0.61      0.28    401176\n",
      "weighted avg       0.93      0.30      0.38    401176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print evaluation results\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_undersampled))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_undersampled))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_undersampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy dropped from 92.76% to 29.73%. This drop is expected due to balancing the dataset.\n",
    "\n",
    "Our precision from our baseline model to our resampled balanced model went from 0.67 to 0.09, meaning we have more false positives now. Our recall jumped from 0 to 0.99 and our F1-score also increased from 0 to 0.17.\n",
    "\n",
    "We have 90,673 true negatives, meaning these are the cases our model correctly predicted \"NOT FAILED TO YIELD\". We have 281,479 false positives, meaning our model incorrectly predicted \"FAILED TO YIELD\" when it was actually \"NOT FAILED TO YIELD\". We have 398 false negatives, meaning our model incorrectly predicted \"NOT FAILED TO YIELD\" when it was actually \"FAILED TO YIELD\". And we have 28,626 true positives, meaning these are all the cases that were correctly predicted as \"FAILED TO YIELD\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use a decision tree to iterate on our model. We'll be using a decision tree because it is non-linear and can capture complex relationships in data, handles class imbalance better, and it is easily interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight='balanced', max_depth=10, random_state=42)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Decision Tree Classifier\n",
    "dt_model = DecisionTreeClassifier(random_state=42, max_depth=10, class_weight=\"balanced\")\n",
    "\n",
    "# Train the model on the resampled (undersampled) dataset\n",
    "dt_model.fit(X_train_resampled, y_train_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_dt = dt_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2923704309330568\n",
      "Confusion Matrix:\n",
      " [[ 88546 283606]\n",
      " [   278  28746]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.24      0.38    372152\n",
      "           1       0.09      0.99      0.17     29024\n",
      "\n",
      "    accuracy                           0.29    401176\n",
      "   macro avg       0.54      0.61      0.28    401176\n",
      "weighted avg       0.93      0.29      0.37    401176\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_dt))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our decision tree model performed very similarly to our resampled model. The decision tree model's accuracy is 29.23% and the undersampled model's is 29.73%. The precision, recall and f1-score all remained the same as our undersampled model. Our false positive count from our undersampled model to our decision tree model jumped from 281,479 to 283,606. This model did not improve significantly from our logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vehicle Safety Board of Chicago is likely focused on identifying as many \"FAILED TO YIELD\" caes as possible (high recall) and minimizing incorrect classifications of \"FAILED TO YIELD\" (high precision), therefore having a nice balance between the two metrics. This would mean our F1-score is the strongest metric our board cares about. \n",
    "\n",
    "After testing multiple models, our resampled logistic regression model remains the best choice for the Vehicle Safety Board of Chicago. This model increased F1-score from 0 (baseline) to 0.17, showing a significant improvement in balancing recall and precision. Our baseline model was useless for safety analysis, as its recall was 0, meaning it completely ignored all \"FAILED TO YIELD\" cases. The board cannot make policy recommendations if the model fails to detect real incidents.\n",
    "\n",
    "We also tested a Decision Tree Classifier to see if it could provide a better balance. While the Decision Tree maintained a high recall (0.99), it did not improve precision or F1-score, producing results nearly identical to the undersampling logistic regression model. Additionally, the Decision Tree had slightly more false positives, which further confirms logistic regression with undersampling remains the best option.\n",
    "\n",
    "Moving forward, improving precision without sacrificing recall should be the focus of further iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vehicle Safety Board of Chicago should incorporate GIS (Geographic Information System) data to examind the spatial distribution of the accidents. By analyzing latitude and longitude coordinates we can identify high-risk locations where \"FAILED TO YIELD\" accidents frequently occur. Using spatial clustering or heat map analyses we can rank locations based on accident frequency. After finding out these high traffic areas, we could assess traffic flow patterns, road design issues and driver behavior.\n",
    "\n",
    "From these results we could make evidence-based solutions to decreasing frequency of these accidents. By integrating GIS data into our machine learning model, we can strengthen policy recommendations and develop targeted interventions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
